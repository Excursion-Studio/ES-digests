---
title: "自注意力机制详解"
---

自注意力机制（Self-Attention）是 Transformer 的核心创新。

## 基本原理

自注意力机制允许模型在处理序列时，同时考虑所有位置的信息，而不是像 RNN 那样逐步处理。

## 计算过程

对于输入序列中的每个位置，自注意力机制会：

1. 计算 **查询（Query）**、**键（Key）**、**值（Value）** 三个向量
2. 用 Query 与所有 Key 做点积，得到注意力权重
3. 用注意力权重对 Value 进行加权求和

## 公式

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$$

## 优势

- **并行计算**：所有位置可以同时计算
- **长距离依赖**：任意两个位置之间都可以直接交互
- **可解释性**：注意力权重可以可视化
